{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a126cca-808d-419a-bc0d-eaf6110e3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a6f13b-af29-4cd8-a683-7ee4a9f892f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4bef71-14b9-40e4-981a-019d9b81c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d534c46e-c9a7-493b-b7bb-65208e76e3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# loads variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae2d212f-c2f0-4cb1-b686-7c0d7c6619b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secret loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if GOOGLE_API_KEY is None:\n",
    "    raise ValueError(\"Environment variable GOOGLE_API_KEY not found!\")\n",
    "\n",
    "print(\"Secret loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4719892c-8a56-4532-a435-a76b5d7eba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, imagine you have a really, really smart puppy. This puppy can learn things, but you have to teach it first.\n",
      "\n",
      "That puppy is like AI, or Artificial Intelligence. \"Artificial\" means not real, like a fake tree. \"Intelligence\" means being able to learn and understand things, like you learning how to ride your bike!\n",
      "\n",
      "So, AI is like a brain that's not inside a real person or animal. It's made with computers!\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "*   **You show it lots of examples:** Imagine you want to teach the puppy to recognize a cat. You show it hundreds of pictures of cats.\n",
      "*   **It learns the patterns:** The puppy looks at all those pictures and figures out what makes a cat a cat: pointy ears, whiskers, a tail, etc. The computer does this too!\n",
      "*   **It can do new things:** Now, when you show the puppy a new picture, it can say, \"That's a cat!\" even if it's never seen that specific cat before. The computer can do the same with new information it's given.\n",
      "\n",
      "AI can do lots of cool things:\n",
      "\n",
      "*   **Help you find videos:** Like when you ask your tablet to play your favorite cartoon.\n",
      "*   **Play games:** Some video game characters are controlled by AI.\n",
      "*   **Help doctors find diseases:** AI can look at lots of medical information to help doctors figure out what's wrong with people.\n",
      "*   **Drive cars (someday!):** Some cars are learning to drive themselves using AI.\n",
      "\n",
      "So, AI is like a super-smart computer that can learn and do cool things, like recognize cats, play games, and maybe even drive your car one day!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Explain AI to me like I'm a kid.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "222731b8-c8ec-401e-873f-3407e1f11a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, imagine you have a really, really smart puppy. This puppy can learn things, but you have to teach it first.\n",
       "\n",
       "That puppy is like AI, or Artificial Intelligence. \"Artificial\" means not real, like a fake tree. \"Intelligence\" means being able to learn and understand things, like you learning how to ride your bike!\n",
       "\n",
       "So, AI is like a brain that's not inside a real person or animal. It's made with computers!\n",
       "\n",
       "Here's how it works:\n",
       "\n",
       "*   **You show it lots of examples:** Imagine you want to teach the puppy to recognize a cat. You show it hundreds of pictures of cats.\n",
       "*   **It learns the patterns:** The puppy looks at all those pictures and figures out what makes a cat a cat: pointy ears, whiskers, a tail, etc. The computer does this too!\n",
       "*   **It can do new things:** Now, when you show the puppy a new picture, it can say, \"That's a cat!\" even if it's never seen that specific cat before. The computer can do the same with new information it's given.\n",
       "\n",
       "AI can do lots of cool things:\n",
       "\n",
       "*   **Help you find videos:** Like when you ask your tablet to play your favorite cartoon.\n",
       "*   **Play games:** Some video game characters are controlled by AI.\n",
       "*   **Help doctors find diseases:** AI can look at lots of medical information to help doctors figure out what's wrong with people.\n",
       "*   **Drive cars (someday!):** Some cars are learning to drive themselves using AI.\n",
       "\n",
       "So, AI is like a super-smart computer that can learn and do cool things, like recognize cats, play games, and maybe even drive your car one day!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888bf34-afa0-402b-b478-e9697bc81865",
   "metadata": {},
   "source": [
    "## Multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e352705f-9669-4f14-8f32-596c20955e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings, Zlork! It's nice to meet you. Is there anything I can help you with today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('Hello! My name is Zlork.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5d08fc-0e83-411d-b605-e45d7d7a85fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a fascinating fact about dinosaurs:\n",
      "\n",
      "**Some dinosaurs, like the Microraptor, may have had four wings!**\n",
      "\n",
      "Microraptor was a small, feathered dinosaur that lived during the Early Cretaceous period. Fossils show that it had feathers on both its forelimbs and hindlimbs, forming what are essentially two pairs of wings. While it probably couldn't fly as efficiently as modern birds, it likely used these four wings for gliding between trees and maneuvering in the air.\n",
      "\n",
      "Isn't that wild? A four-winged dinosaur! It really challenges our traditional image of what dinosaurs were like.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell me something interesting about dinosaurs?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eef0f8b-fbd4-479b-80d4-e6c7e89fac28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Zlork. I remember your name is Zlork.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b471be0-c0a8-41f1-9b78-210fc356922b",
   "metadata": {},
   "source": [
    "## Checking available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e2861e4-2b06-4c35-b9ce-3df6963e7123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-preview\n",
      "models/veo-3.0-fast-generate-preview\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2124a06e-5bf9-412b-972e-32e175bfd744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Gemini 2.0 Flash',\n",
      " 'display_name': 'Gemini 2.0 Flash',\n",
      " 'input_token_limit': 1048576,\n",
      " 'name': 'models/gemini-2.0-flash',\n",
      " 'output_token_limit': 8192,\n",
      " 'supported_actions': ['generateContent',\n",
      "                       'countTokens',\n",
      "                       'createCachedContent',\n",
      "                       'batchGenerateContent'],\n",
      " 'tuned_model_info': {},\n",
      " 'version': '2.0'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    pprint(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfce7c9-a44d-4408-8484-e73b3fdcfee4",
   "metadata": {},
   "source": [
    "## Explore generation parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6f32f52-6dbc-42f4-90fc-f8c617e56d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The humble olive, a fruit cultivated for millennia, continues to hold a significant place in modern society. Beyond its Mediterranean origins, the olive and its derivatives, particularly olive oil, are now staples in global cuisine and contribute significantly to diverse sectors.\n",
      "\n",
      "Olive oil is celebrated for its health benefits, lauded for its monounsaturated fats and antioxidants. Its role in the Mediterranean diet, associated with longevity and reduced risk of chronic diseases, has solidified its position as a healthy cooking oil. This has driven increased demand, fueling a thriving industry that spans from agriculture and processing to export and distribution.\n",
      "\n",
      "Furthermore, olives themselves are a versatile food. Eaten whole, stuffed, or incorporated into dishes, they add a unique briny flavor appreciated worldwide. They feature prominently in tapas, salads, and numerous Mediterranean and Middle Eastern recipes, highlighting their adaptability across diverse culinary traditions.\n",
      "\n",
      "Beyond food, olives contribute to industries like cosmetics and pharmaceuticals. Olive oil is used in skincare products for its moisturizing and emollient properties.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Here we limit the amount of output tokens\n",
    "short_config = types.GenerateContentConfig(max_output_tokens=200)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Write a 200 word essay on the importance of olives in modern society.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "495fae29-0e7c-45bf-9759-dae438a92bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From groves of sun, a gift descends,\n",
      "The humble olive, modern trends.\n",
      "In oil that gleams, a healthy pour,\n",
      "On salads bright, and sauces' core.\n",
      "\n",
      "Tapenade's zest, a pizza's art,\n",
      "A global flavor, set apart.\n",
      "No longer just a Grecian dream,\n",
      "But woven in our culinary scheme.\n",
      "\n",
      "So raise a glass, to salty bite,\n",
      "The olive's reign, both dark and bright.\n",
      "A symbol strong, of peace and taste,\n",
      "A modern marvel, never waste.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=short_config,\n",
    "    contents='Write a short poem on the importance of olives in modern society.')\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade41735-2d51-4f74-9634-5d5730422fca",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eebca140-f5ee-4ed8-a49f-7422bcbb5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple\n",
      " -------------------------\n",
      "Turquoise\n",
      " -------------------------\n",
      "Cerulean\n",
      " -------------------------\n",
      "Turquoise\n",
      " -------------------------\n",
      "Azure\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "high_temp_config = types.GenerateContentConfig(temperature=2.0)\n",
    "# Temp 0.0 will always give Azure\n",
    "# Temp 0.5 gives mostly Azure but sometimes other colors.\n",
    "# Temp 2.0 gives almost always something different.\n",
    "\n",
    "for _ in range(5):\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=high_temp_config,\n",
    "      contents='Pick a random colour... (respond in a single word)')\n",
    "\n",
    "  if response.text:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31197992-c43e-42f9-9b6e-da25d137b740",
   "metadata": {},
   "source": [
    "### Top-P\n",
    "\n",
    "Like temperature, the top-P parameter is also used to control the diversity of the model's output.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "You may also see top-K referenced in LLM literature. Top-K is not configurable in the Gemini 2.0 series of models, but can be changed in older models. Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ddd0446c-4783-4da8-9974-35abaab5ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clementine was, by all accounts, a pampered Persian. She lived in a sun-drenched apartment filled with plush velvet pillows, a scratching post shaped like a miniature Eiffel Tower, and a human named Eleanor who catered to her every whim. But Clementine yearned for more than tuna pate and feather wands. She yearned for adventure.\n",
      "\n",
      "One particularly dull afternoon, while Eleanor was engrossed in a book about Victorian embroidery, Clementine spotted it: a crack in the living room window, a sliver of the outside world beckoning. A world of rustling leaves, chirping birds, and… possibilities.\n",
      "\n",
      "With a burst of unexpected agility, Clementine squeezed through the opening. The cool air nipped at her fur, carrying the scent of damp earth and something tantalizingly unknown. This was it. Her adventure.\n",
      "\n",
      "Her first challenge was the fire escape. The metal was cold and unforgiving, but Clementine, fueled by her newfound freedom, navigated it with surprising grace. She descended, her fluffy tail acting as a counterweight, until she reached the alleyway below.\n",
      "\n",
      "The alley was a symphony of smells: overflowing garbage bins, stray dogs, and something vaguely fishy that made her whiskers twitch. She cautiously ventured forward, her paws padding softly on the grimy pavement.\n",
      "\n",
      "Her journey took her past bustling street vendors, their calls a cacophony of sounds. She narrowly avoided being trampled by a rogue skateboarder. She even glimpsed a rat the size of her head, which she wisely chose to ignore.\n",
      "\n",
      "Then, she saw it. A park, a veritable oasis of green in the concrete jungle. Drawn by the promise of trees to climb and squirrels to chase (from a safe distance, of course), she crept through the iron gates.\n",
      "\n",
      "The park was alive with activity. Children shrieked with laughter, dogs barked greetings, and elderly men played chess with focused intensity. Clementine felt a pang of loneliness. She was an outsider, a stranger in a strange land.\n",
      "\n",
      "Suddenly, a small hand reached out to her. A little girl with bright, curious eyes offered Clementine a dandelion. \"Hello, kitty,\" she whispered. Clementine, usually aloof, rubbed against the girl's hand, purring softly.\n",
      "\n",
      "They spent the afternoon together. The girl, whose name was Lily, told Clementine stories about fairies and dragons, and Clementine, in turn, shared the silence and comfort only a cat can offer. For the first time, Clementine felt a connection, a sense of belonging that went beyond her pampered existence.\n",
      "\n",
      "As dusk began to settle, a flicker of anxiety stirred within Clementine. Eleanor would be worried. With a final purr and a gentle headbutt, she bid Lily farewell.\n",
      "\n",
      "The journey back was easier. She navigated the alleyway with practiced ease, climbed the fire escape with newfound confidence, and squeezed back through the window just as Eleanor was starting to call her name.\n",
      "\n",
      "Eleanor scooped her up, burying her face in Clementine’s soft fur. “Clementine! Where have you been? I was so worried!”\n",
      "\n",
      "Clementine simply purred, nuzzling against Eleanor’s neck. She didn’t tell her about the adventure, about the sights and smells and the little girl with the bright eyes. She didn’t need to. She carried the memory with her, a secret treasure that added a new layer to her pampered life.\n",
      "\n",
      "Clementine was still a pampered Persian, but now she was also an adventurer. And sometimes, when the city was quiet and the moon was full, she would sit by the window, gazing out at the world, dreaming of her next great escape. Because even a pampered Persian, needs a little bit of wild in her soul.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    # These are the default values for gemini-2.0-flash.\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=story_prompt,\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac6505-723d-4735-b2bf-42af8e8473ce",
   "metadata": {},
   "source": [
    "_Temperature scales the probabilities of all potential next words, making extreme choices more or less likely, controlling overall creativity. In contrast, Top-P dynamically selects a subset of tokens (the \"nucleus\") whose cumulative probabilities add up to 'P', focusing generation on the most confident, relevant words while cutting off less probable, often irrelevant, tails. Use Temperature for broad creativity/conservatism, and Top-P for dynamically adjusting the pool of acceptable words for coherent diversity._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc70ed6-42c3-4178-8304-a68e782242d1",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a03be-2b73-4f07-9159-fe1dff5f3299",
   "metadata": {},
   "source": [
    "### Zero-shot\n",
    "\n",
    "Zero-shot prompts are prompts that describe the request for the model directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a6e4c8a4-591a-4393-844f-494f08f8989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = types.GenerateContentConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=5,\n",
    ")\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. We should be cautiously optimistic.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=model_config,\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3d7fb-e66a-485b-8616-6a9543c9449f",
   "metadata": {},
   "source": [
    "#### Enum mode\n",
    "\n",
    "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards. See [this prompt in AI Studio](https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g) for an example.\n",
    "\n",
    "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows you to constrain the output to a fixed set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "755a8008-c1a7-4512-8b5e-f37a18d0afd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ),\n",
    "    contents=zero_shot_prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "8c05eeb2-482e-4bd0-9a23-a0ef0b7a6a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment.NEUTRAL\n",
      "<enum 'Sentiment'>\n"
     ]
    }
   ],
   "source": [
    "enum_response = response.parsed\n",
    "print(enum_response)\n",
    "print(type(enum_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342bb1b-b7d7-4ab1-b66d-f25bb2e70fab",
   "metadata": {},
   "source": [
    "## One-Shot and Few-Shot\n",
    "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.\n",
    "\n",
    "```\n",
    "Classify the sentiment of the following text as positive, negative, or neutral.\n",
    "Text: The product is terrible.\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: I think the vacation was okay. Sentiment:\n",
    "```\n",
    "This is one shot as one example was provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd0f179-179c-4c44-9393-f78eb2061720",
   "metadata": {},
   "source": [
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "```\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ),\n",
    "    contents=[few_shot_prompt, customer_order])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2d528-89d1-42c7-b394-0fe0ea7e215c",
   "metadata": {},
   "source": [
    "#### JSON mode\n",
    "\n",
    "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "39d7b9f7-d85b-4ce9-8da6-3dacb1df91d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
      "  \"type\": \"dessert\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ),\n",
    "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df78a7-e017-4fac-9fe7-a7c32b46caa1",
   "metadata": {},
   "source": [
    "### Chain of Thought (CoT)\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0f8d250c-7083-4b8b-8009-171f667331ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b79535-7f8f-457b-8def-1db60961485c",
   "metadata": {},
   "source": [
    "The expected answer above is 28. When they were 4 years old, the partner was 3*4 = 12, an 8 year difference.\n",
    "Now the person is 20 years old, so the partner must be 28 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a700c5b7-c50a-473c-b1a9-e8552bfe04e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "1.  **Find the age difference:** When you were 4, your partner was 3 times your age, so they were 4 * 3 = 12 years old.\n",
       "\n",
       "2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
       "\n",
       "3.  **Determine your partner's current age:** Since you are now 20 and your partner is 8 years older, they are currently 20 + 8 = 28 years old.\n",
       "\n",
       "**Answer:** Your partner is 28 years old."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8a3e9-25a9-4f66-b093-eabf0d212f65",
   "metadata": {},
   "source": [
    "### ReAct: Reason and act\n",
    "\n",
    "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper.\n",
    "\n",
    "To try this out with the Wikipedia search engine, check out the [Searching Wikipedia with ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) cookbook example.\n",
    "\n",
    "\n",
    "> Note: The prompt and in-context examples used here are from [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct) which is published under an [MIT license](https://opensource.org/licenses/MIT), Copyright (c) 2023 Shunyu Yao.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/18oo63Lwosd-bQ6Ay51uGogB3Wk3H8XMO\">Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1dcaec01-5e17-494b-a068-80e35d7d6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "46efdef9-150c-4c31-a204-e25a51122e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to find the transformers NLP paper and identify the youngest author listed on it.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "# You will perform the Action; so generate up to, but not including, the Observation.\n",
    "react_config = types.GenerateContentConfig(\n",
    "    stop_sequences=[\"\\nObservation\"],\n",
    "    system_instruction=model_instructions + example1 + example2,\n",
    ")\n",
    "\n",
    "# Create a chat that has the model instructions and examples pre-seeded.\n",
    "react_chat = client.chats.create(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=react_config,\n",
    ")\n",
    "\n",
    "resp = react_chat.send_message(question)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d1c4074c-de2b-4d7a-89f8-0cb6a2163dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "The paper lists the authors Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the youngest among them. This would likely involve finding their birthdates, which would require multiple searches. I will start with the first author, Ashish Vaswani.\n",
      "\n",
      "Action 2\n",
      "<search>Ashish Vaswani</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff001c80-90b9-4d27-a5f1-0938726d16a0",
   "metadata": {},
   "source": [
    "## Thinking mode\n",
    "\n",
    "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
    "\n",
    "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.\n",
    "\n",
    "Note that when you use the API, you get the final response from the model, but the thoughts are not captured. To see the intermediate thoughts, try out [the thinking mode model in AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-thinking-exp-01-21).\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1Z991SV7lZZZqioOiqIUPv9a9ix-ws4zk\">Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6768c7ca-f1d9-471e-9c66-badb2b3cbe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The youngest author listed on the \"Attention Is All You Need\" paper (which introduced the Transformer model) was **Aidan N. Gomez**.\n",
       "\n",
       "He was 20 years old when the paper was published in June 2017. At the time, he was a PhD student at the University of Toronto."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from IPython.display import Markdown, clear_output\n",
    "\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model='gemini-2.0-flash-thinking-exp',\n",
    "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
    ")\n",
    "\n",
    "buf = io.StringIO()\n",
    "for chunk in response:\n",
    "    buf.write(chunk.text)\n",
    "    # Display the response as it is streamed\n",
    "    print(chunk.text, end='')\n",
    "\n",
    "# And then render the finished response as formatted markdown.\n",
    "clear_output()\n",
    "Markdown(buf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc30fa-f10b-4404-8a16-24cc49accbc1",
   "metadata": {},
   "source": [
    "## Code Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8478ab35-1e87-4312-bb83-d613809252f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  \"\"\"Calculate the factorial of a number.\"\"\"\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Gemini models love to talk, so it helps to specify they stick to the code if that\n",
    "# is all that you want.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=code_prompt)\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ac2f4e2e-cae1-40e4-a6c0-09c443b3ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\ndef factorial(n):\\n  \"\"\"Calculate the factorial of a number.\"\"\"\\n  if n == 0:\\n    return 1\\n  else:\\n    return n * factorial(n-1)\\n```'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the markdown format\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143a158-11f6-4f99-aae8-bad57fa12ced",
   "metadata": {},
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API can automatically run generated code too, and will return the output.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\">Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b7712a26-cc9e-416d-a52a-b0a7243867ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Okay, I can do that. First, I need to generate the first 14 odd '\n",
      "         'prime numbers. A prime number is a number greater than 1 that has '\n",
      "         'only two factors: 1 and itself. Odd prime numbers are prime numbers '\n",
      "         'that are not divisible by 2.\\n'\n",
      "         '\\n'\n",
      "         \"Here's how I'll approach this: I'll start listing odd numbers and \"\n",
      "         'checking if they are prime.\\n'\n",
      "         '\\n'\n",
      "         '1.  3 is prime.\\n'\n",
      "         '2.  5 is prime.\\n'\n",
      "         '3.  7 is prime.\\n'\n",
      "         '4.  9 is not prime (3\\\\*3).\\n'\n",
      "         '5.  11 is prime.\\n'\n",
      "         '6.  13 is prime.\\n'\n",
      "         '7.  15 is not prime (3\\\\*5).\\n'\n",
      "         '8.  17 is prime.\\n'\n",
      "         '9.  19 is prime.\\n'\n",
      "         '10. 21 is not prime (3\\\\*7).\\n'\n",
      "         '11. 23 is prime.\\n'\n",
      "         '12. 25 is not prime (5\\\\*5).\\n'\n",
      "         '13. 27 is not prime (3\\\\*9).\\n'\n",
      "         '14. 29 is prime.\\n'\n",
      "         '15. 31 is prime.\\n'\n",
      "         '16. 33 is not prime (3\\\\*11).\\n'\n",
      "         '17. 35 is not prime (5\\\\*7).\\n'\n",
      "         '18. 37 is prime.\\n'\n",
      "         '19. 39 is not prime (3\\\\*13).\\n'\n",
      "         '20. 41 is prime.\\n'\n",
      "         '21. 43 is prime.\\n'\n",
      "         '22. 45 is not prime (5\\\\*9).\\n'\n",
      "         '23. 47 is prime.\\n'\n",
      "         '\\n'\n",
      "         'So, the first 14 odd prime numbers are: 3, 5, 7, 11, 13, 17, 19, 23, '\n",
      "         '29, 31, 37, 41, 43, 47.\\n'\n",
      "         '\\n'\n",
      "         \"Now, let's calculate the sum of these numbers using python.\\n\"\n",
      "         '\\n'}\n",
      "-----\n",
      "{'executable_code': {'code': 'import numpy as np\\n'\n",
      "                             'primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n",
      "                             '37, 41, 43, 47]\\n'\n",
      "                             'sum_of_primes = np.sum(primes)\\n'\n",
      "                             'print(sum_of_primes)\\n'\n",
      "                             '\\n',\n",
      "                     'language': 'PYTHON'}}\n",
      "-----\n",
      "{'code_execution_result': {'outcome': 'OUTCOME_OK', 'output': '326\\n'}}\n",
      "-----\n",
      "{'text': 'Therefore, the sum of the first 14 odd prime numbers is 326.\\n'}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
    ")\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Generate the first 14 odd prime numbers, then calculate their sum.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=config,\n",
    "    contents=code_exec_prompt)\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  pprint(part.to_json_dict())\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0aad3-221c-446a-bb58-4d641d74a235",
   "metadata": {},
   "source": [
    "This response contains multiple parts, including an opening and closing text part that represent regular responses, an `executable_code` part that represents generated code and a `code_execution_result` part that represents the results from running the generated code.\n",
    "\n",
    "You can explore them individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f271a4fc-add4-4e9f-85fd-b830509753e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, I can do that. First, I need to generate the first 14 odd prime numbers. A prime number is a number greater than 1 that has only two factors: 1 and itself. Odd prime numbers are prime numbers that are not divisible by 2.\n",
       "\n",
       "Here's how I'll approach this: I'll start listing odd numbers and checking if they are prime.\n",
       "\n",
       "1.  3 is prime.\n",
       "2.  5 is prime.\n",
       "3.  7 is prime.\n",
       "4.  9 is not prime (3\\*3).\n",
       "5.  11 is prime.\n",
       "6.  13 is prime.\n",
       "7.  15 is not prime (3\\*5).\n",
       "8.  17 is prime.\n",
       "9.  19 is prime.\n",
       "10. 21 is not prime (3\\*7).\n",
       "11. 23 is prime.\n",
       "12. 25 is not prime (5\\*5).\n",
       "13. 27 is not prime (3\\*9).\n",
       "14. 29 is prime.\n",
       "15. 31 is prime.\n",
       "16. 33 is not prime (3\\*11).\n",
       "17. 35 is not prime (5\\*7).\n",
       "18. 37 is prime.\n",
       "19. 39 is not prime (3\\*13).\n",
       "20. 41 is prime.\n",
       "21. 43 is prime.\n",
       "22. 45 is not prime (5\\*9).\n",
       "23. 47 is prime.\n",
       "\n",
       "So, the first 14 odd prime numbers are: 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47.\n",
       "\n",
       "Now, let's calculate the sum of these numbers using python.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import numpy as np\n",
       "primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum_of_primes = np.sum(primes)\n",
       "print(sum_of_primes)\n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "326\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Therefore, the sum of the first 14 odd prime numbers is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text:\n",
    "        display(Markdown(part.text))\n",
    "    elif part.executable_code:\n",
    "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
    "    elif part.code_execution_result:\n",
    "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
    "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
    "\n",
    "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65851bf0-c146-4e38-8553-5e2abc949c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
